# Config file for MBHB run. Vary only 2 parameters: m1 and m2.
# Gaussian base distribution with resampling.
# Data is generated on the fly.

model:                                 # Properties of the model

    base:                              # Base distribution
      gaussian: 0                      # Type of the base distribution, can be either gaussian (1) or resampled (0) 
      learn_mean_var: False            # Flag, if true, mean and variance of the distribution is learned
      params: 8                        # Number of the physical parameters that we want to fit 
    
    acceptance_fn:                     # These parameters are only for the case when we use base resampling
      # To be implemented

    transform:
      type: rq-coupling                # Type of the transform blocks. At the moment use just one but should be extended to more in the future.
      hidden_features: 512             
      num_blocks: 12 #8                   # Number of times the flow transformation is applied.
      dropout: 0.1
      batch_norm: 1
      num_bins: 3
      tail_bound: 5                    # Box is on [-bound, bound]^2 Can be from 1 to 5
      unconditional_transform: False   # Apply unconditional transform
      activation: elu

    flow:
      num_flow_steps: 14              # Number of times the flow transformation is applied

    # TODO: there is no context for GBs, we just take the waveforms as it is and compress them with the embedding network
    # This has to be made equal to the size of the input
    context:
      coeffs: 512 #12616                       # Number of the first coefficients taken after projecting the data on the new basis, acquired with the SVD decomposition
      context_features: 68                     # Size of the context data after embedding

    embedding:
      type: residual
      hidden_features: 18  #256
      num_blocks: 8
      activation: elu
      dropout: 0.01 
      batch_norm: 1

training:

    batch_size: 2048                   # Batch size
    epochs: 1000                       # Number of epochs
    max_iter: 500                      # Number of iterations inside one ephoch, useful only when the data is created on the fly 
    learning_rate: 2.e-4               # Initial learning rate.
    anneal_learning_rate: 1            # Adjust learnng rate with the cosine annealing. 1 -- TRUE, 0 -- FALSE
    num_training_steps: 50000          # Number of total training steps (important for annealing of the learning rate)
    weight_decay: 1.e-5                # Parameter used in the optimiser
    seed: 1                            # Seed for training run
    grad_norm_clip_value: 5            # Value by which to clip the norm of the gradient 
    resume: 0                          # Resume training (1) or start from scratch (0).
    checkpoints: checkpoint_1.pt       # Path to the checkpoint from which we are going to resume the training

saving:

    save_root: /sps/lisaf/natalia/training_data/test_gb_frq
    label: resample_test

plots:

    label: freq
