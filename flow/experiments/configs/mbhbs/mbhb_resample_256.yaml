# Config file for MBHB run. Vary only 2 parameters: m1 and m2.
# Gaussian base distribution with resampling.
# Data is generated on the fly.

model:                                 # Properties of the model

    base:                              # Base distribution
      gaussian: 0                      # Type of the base distribution, can be either gaussian (1) or resampled (0) 
      learn_mean_var: False            # Flag, if true, mean and variance of the distribution is learned
      params: 10                        # Number of the physical parameters that we want to fit 
    
    acceptance_fn:                     # These parameters are only for the case when we use base resampling
      # To be implemented

    transform:
      type: rq-coupling                # Type of the transform blocks. At the moment use just one but should be extended to more in the future.
      hidden_features: 512             
      num_blocks:  8 #12                    # Number of times the flow transformation is applied.
      dropout: 0.1 # 0.05
      batch_norm: 1
      num_bins: 5
      tail_bound: 5                    # Box is on [-bound, bound]^2 Can be from 1 to 5
      unconditional_transform: False   # Apply unconditional transform
      activation: relu

    flow:
      num_flow_steps: 10 #12 #18 #16               # Number of times the flow transformation is applied

    context:
      coeffs: 128                       # Number of the first coefficients taken after projecting the data on the new basis, acquired with the SVD decomposition
      context_features: 128           # Size of the context data after embedding

    embedding:
      type: residual
      hidden_features: 128 # 256
      num_blocks: 8 #8 #12 #22
      activation: relu # relu
      dropout: 0.05 
      batch_norm: 1

training:

    optimizer: Adam                    # Adam, SDG, Hessian
    batch_size: 1024 #2048                   # Batch size
    epochs: 500                       # Number of epochs
    max_iter: 500                      # Number of iterations inside one ephoch, useful only when the data is created on the fly 
    learning_rate: 2.e-4               # Initial learning rate.
    anneal_learning_rate: 1            # Adjust learnng rate with the cosine annealing. 1 -- TRUE, 0 -- FALSE
    num_training_steps: 500            # Number of total training steps (important for annealing of the learning rate)
    weight_decay: 1.e-5                # Parameter used in the optimiser
    seed: 1                            # Seed for training run
    grad_norm_clip_value: 5 # 5           # Value by which to clip the norm of the gradient 
    resume: 0                          # Resume training (1) or start from scratch (0).
    checkpoints: checkpoint_87.pt      # Path to the checkpoint from which we are going to resume the training

svd:

    root: ./
    path: SVD_128_10_SGD.hdf5
    estimate: 1
    method: ipca


saving:

    save_root: /sps/lisaf/natalia/training_data/SVD/mbhb_SGD/
    label: _SGD

plots:

    label: _SGD
